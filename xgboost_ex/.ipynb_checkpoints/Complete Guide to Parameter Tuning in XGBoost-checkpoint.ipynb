{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# XGBoost调参完全指南\n",
    "***\n",
    "[原文链接](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ \"Complete Guide to Parameter Tuning in XGBoost\")\n",
    "\n",
    "## 简介\n",
    "***\n",
    "如果在预测时, 结果不是你预期的样子, 那么使用 XGBoost 吧! XGBoost 算法已经成为许多数据科学家的终极武器. 这是一个高度复杂的算法, 强大到足以处理各种不规则的数据.\n",
    "\n",
    "使用 XGBoost 训练一个模型很容易. 但是, 进一步改进模型却很困难(至少我花费了很多时间). 这个算法有很多参数. 为了改进模型, 参数调节是必不可少的. 对于像\"哪些参数你应该调节? 为了获得最优结果, 这些参数的理想值是多少?\"这样的实际问题很难找到答案. \n",
    "\n",
    "这篇文章很适合 XGBoost 新手. 在文章中, 我们将学习**调参的艺术**和一些与 XGBoost 相关的有用信息. 当然, 我们也会在 Python 中实践一下这个算法.\n",
    "\n",
    "## 目录\n",
    "***\n",
    "1. [XGBoost的优点](#1)\n",
    "2. [XGBoost的参数](#2)\n",
    "3. [调参实例](#3)\n",
    "\n",
    "<h2 id=\"1\"> 1. XGBoost的优点 </h2>\n",
    "***\n",
    "当我在探究算法的性能和其高精度背后的内在原因时, 我发现了很多优点:\n",
    "\n",
    "1. **正则化**:\n",
    "    * [正则化](https://www.analyticsvidhya.com/blog/2015/02/avoid-over-fitting-regularization/)能够减少过拟合, 而标准的 GBM 实现没有与 XGBoost 类似的正则化.\n",
    "    * XGBoost 也被称为: 带正则化的提升技巧(**regularized boosting technique**).\n",
    "    \n",
    "2. **并行处理**:\n",
    "    * XGBoost 实现了并行计算, 相比于 GBM 更快.\n",
    "    * 但是等一下, 我们知道 [boosting](https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/) 算法是顺序生成子估计器的, 那么它是如何并行化的呢? 我们知道每一颗树只有在前一颗树生成之后才能被构建,那么是什么原因阻止我们用多核生成树呢? 关于并行化进一步参考[Parallel Gradient Boosting Decision Trees](http://zhanpengfang.github.io/418home.html).\n",
    "    * XGBoost 支持 Hadoop.\n",
    "    \n",
    "3. **高灵活性**:\n",
    "    * XGBoost 允许用户去自定义目标函数(损失函数)和评价指标.\n",
    "    * 允许使用者自定义给模型添加了一个全新的维度.\n",
    "    \n",
    "4. **处理缺失值**:\n",
    "    * XGBoost 有一个内建的程序去处理缺失值.\n",
    "    * 使用者除了提供训练数据之外, 还需要提供一个不同的值作为函数的参数. 在遇到缺失值的节点上, XGBoost 会尝试不同的方法, 然后学会采用何种方法处理缺失值, 这样就能应对将来的缺失情况.\n",
    "    \n",
    "5. **剪枝**:\n",
    "    * GBM 在生成节点时, 如果遇到一次划分得到负的损失(negative loss), 算法就会停止划分. 因此, GBM 更多是一个贪心算法.\n",
    "    * XGBoost 则是采用先按指定的 `max_depth` 生成树, 然后从叶子节点向根节点剪枝, 剪枝会去掉不是正增益的划分.\n",
    "    * 另一个优点是: 有时一次负增益划分(比方说-2)后紧跟一次正增益划分(比方说10), GBM 会在遇到负增益时停止划分, 但是 XGBoost 会看到组合增益是8而继续划分.\n",
    "\n",
    "6. **內建交叉验证**:\n",
    "    * XGBoost 允许使用者在提升过程的每一次迭代运行交叉验证, 因此, 这让你能够在单次运行后得到准确的最优迭代轮数(子估计器的个数).\n",
    "    * 不同于 GBM 进行网格搜索(grid-search)只能尝试有限个数的参数.\n",
    "    \n",
    "7. **在训练过的模型上继续训练**:\n",
    "    * 使用者能够在上一次训练模型结果的基础上继续训练模型(warm-start), 该功能在特定应用场景很有用.\n",
    "    * sklearn 库中实现的 GBM 也有这项功能.\n",
    "    \n",
    "官方介绍参考链接:\n",
    "\n",
    "* [XGBoost Guide - Introduction to Boosted Trees](http://xgboost.readthedocs.io/en/latest/model.html)([已阅](./Introduction to Boosted Trees.ipynb))\n",
    "\n",
    "<h2 id=\"2\"> 2. XGBoost的参数 </h2>\n",
    "***\n",
    "XGBoost 的作者将所有的参数分成为三类:\n",
    "1. General Parameters: 调节整体功能\n",
    "2. Booster Parameters: 调节每一个 booster(这里指子估计器)\n",
    "3. Learning Task Parameters: 调节采用的优化方法\n",
    "\n",
    "我将会给出和 GBM 的类比, 所以强烈建议阅读一下[这篇文章](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)学习 GBM 的基础.\n",
    "\n",
    "### General Parameters\n",
    "***\n",
    "这些参数定义了 XGBoost 的整体功能.\n",
    "\n",
    "1. **`booster[default=gbtree]`**\n",
    "    * 选择子估计器的类型, 有两种选择:\n",
    "        * **`gbtree`**: 基于树的模型\n",
    "        * **`gblinear`**: 线性模型\n",
    "\n",
    "2. **`silent[default=0]`**\n",
    "    * 设置为1时, 静音模式(silent mode)被开启(此时, 运行过程消息不会被打印).\n",
    "    * 使用默认值通常是一个好的选择, 因为运行过程消息可能会有用.\n",
    "    \n",
    "3. **`nthread[default to maximum number of threads available if not set]`**\n",
    "    * 设置使用的线程数, 用于并行处理.\n",
    "    * 如果你想同时运行你机器上的所有线程, 采用默认值, 程序会自动检测有几个线程可用.\n",
    "    \n",
    "这里还有两个参数你不需要操心他们, 让我们继续 Booster Parameters.\n",
    "\n",
    "### Booster Parameters\n",
    "***\n",
    "虽然, 有两种类型的 booster 可用, 但是我们这里只讨论 **tree booster**, 因为它的表现通常比 **linear booster** 好, 所以后者很少使用.\n",
    "\n",
    "1. **`eta[default=0.3]`**\n",
    "    * 与 GBM 中的学习率类似\n",
    "    * 通过减小该值能够使模型更稳定\n",
    "    * 最终被使用的典型值是: 0.01-0.2\n",
    "\n",
    "2. **`min_child_weight[default=1]`**\n",
    "    * 节点想要继续划分, 所需的实例权重之和的最小值.\n",
    "    * 这和 GBM 中的 **`min_child_leaf`** 相似, 但是不完全相同. **`min_child_weight`** 指的是实例权重之和的最小值, 而 **`min_child_leaf`** 指的是实例数目的最小值.\n",
    "    * 该参数用来控制过拟合, 太大的值可能会导致欠拟合, 因此, 需要使用交叉验证调参.\n",
    "    \n",
    "3. **`max_depth[default=6]`**\n",
    "    * 单颗树的最大深度.\n",
    "    * 该参数用来控制过拟合, 使用交叉验证调参.\n",
    "    * 典型值: 3-10.\n",
    "    \n",
    "4. **`max_leaf_nodes`**\n",
    "    * 单颗树的叶子节点的最大数.\n",
    "    * 能够用来替代 **`max_depth`**, 因为二叉树的深度和叶子节点个数有确定的函数关系.\n",
    "    * 在 GBM 中, 如果定义了该参数, **`max_depth`** 会被忽略.\n",
    "    \n",
    "5. **`gamma[default=0]`**\n",
    "    * 叶子节点进一步划分所需的最小损失衰减.\n",
    "    * 能够确保算法收敛, 该值根据损失函数的不同而不同.\n",
    "\n",
    "6. **`max_delta_step[default=0]`**\n",
    "    * 每棵树权重估计允许的最大步长增量. 如果采用默认值0, 表示没有约束. 如果设置为正数, 则权重更新时步长更加保守.\n",
    "    * 通常该参数是不需要设置的, 但是在 logistic regression 中, 当数据集类别极度不平衡时可能会有帮助.\n",
    "    * 该参数通常采用默认值, 但是可以进一步探究.\n",
    "    \n",
    "7. **`subsample[default=1]`**\n",
    "    * 训练每棵树时, 采用的训练数据的比例.\n",
    "    * 更小的值使算法更加保守, 从而抑制过拟合, 但是太小的值可能导致欠拟合.\n",
    "    * 典型值: 0.5-1.0.\n",
    "    \n",
    "8. **`colsample_bytree[default=1]`**\n",
    "    * 训练每棵树时, 采用的特征子集的比例.\n",
    "    * 典型值: 0.5-1.0\n",
    "    \n",
    "9. **`colsample_bylevel[default=1]`**\n",
    "    * 节点划分时, 采用的特征子集的比例.\n",
    "    * 我通常不会使用这个参数, 因为设置 **`subsample`** 和 **`colsample_bytree`** 本身对该值就有限制. 有待进一步探究.\n",
    "    \n",
    "10. **`lambda[default=1]`**\n",
    "    * L2 正则化项的权重.\n",
    "    * 尽管很多人不经常使用它, 但是值得被探究用来控制过拟合.\n",
    "    \n",
    "11. **`alpha[default=0]`**\n",
    "    * L1 正则化项的权重.\n",
    "    * 在特征维度很高时采用, 能够加快运行速度.\n",
    "    \n",
    "12. **`scale_pos_weight[default=1]`**\n",
    "    * 该参数平衡正负权重.\n",
    "    * 当遇到正负类别不平衡时, 设置一个大于0的值能够帮助加快收敛速度.\n",
    "    \n",
    "### Learning Task Parameters\n",
    "***\n",
    "这类参数被用来定义优化目标以及每次迭代中应该计算的评价指标. \n",
    "\n",
    "1. **`objective[default=reg:linear]`**\n",
    "    * 该参数用来指定使用的损失函数, 经常使用的有:\n",
    "        * **`binary:logistic`** - 二分类 logistic regression, 返回的是预测概率(不是类别).\n",
    "        * **`multi:softmax`** - 使用 softmax 函数的多分类, 返回预测类别(不是概率).\n",
    "            * 这时需要额外设置一个 **`num_class`** 参数来指定分类类别的个数.\n",
    "        * **`multi:softprob=`** - 与 **`multi:softmax`** 相同, 但是返回的是预测概率.\n",
    "        \n",
    "2. **`eval_metric[default according to objective]`**\n",
    "    * 在验证集上需要计算的指标.\n",
    "    * 回归问题默认是 rmse, 分类问题则是 error.\n",
    "    * 典型值有:\n",
    "        * **`rmse`** - root mean squared error\n",
    "        * **`mae`** - mean absolute error\n",
    "        * **`logloss`** - negative log-likelihood\n",
    "        * **`error`** - Binary classification error rate(0.5 threshold)\n",
    "        * **`merror`** - Multiclass classification error rate\n",
    "        * **`mlogloss`** - Multiclass logloss\n",
    "        * **`auc`** - Area under the curve\n",
    "        \n",
    "3. **`seed[default=0]`**\n",
    "    * 随机数发生器种子.\n",
    "    * 能够用来产生可重复的结果, 也能用来调参.\n",
    "    \n",
    "如果你一直使用 Scikit-Learn 库, 那么这些参数名可能很陌生, 但是 XGBoost 库提供了 sklearn 命名风格的接口(XGBClassifier). 参数对应关系如下:\n",
    "\n",
    "1. eta -> learning_rate\n",
    "2. lambda -> reg_lambda\n",
    "3. alpha -> reg_alpha\n",
    "\n",
    "你可能在想 GBM 中的\"n_estimators\"参数跑哪儿去了, 在 XGBoost 中是\"num_boosting_rounds\"(在调用 fit 函数时传入).\n",
    "\n",
    "参考链接:\n",
    "1. [XGBoost Parameters(official guide)](http://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters)\n",
    "2. [XGBoost Demo Codes(xgboost GitHub repository)](https://github.com/dmlc/xgboost/tree/master/demo/guide-python)\n",
    "3. [Python API Reference(official guide)](http://xgboost.readthedocs.io/en/latest/python/python_api.html)\n",
    "\n",
    "<h2 id=\"3\"> 3. 调参实例 </h2>\n",
    "***\n",
    "我们将会使用来自 Data Hackathon 3.x AV 的数据, 问题的详情见[比赛页面](http://datahack.analyticsvidhya.com/contest/data-hackathon-3x). 你能在[这里](https://www.analyticsvidhya.com/wp-content/uploads/2016/02/Dataset.rar)下载数据集. 该数据集在原数据集上进行了预处理.\n",
    "\n",
    "下面先导入所需的库和加载数据集:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method DMatrix.__del__ of <xgboost.core.DMatrix object at 0x7f5e27855390>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tyj/anaconda3/lib/python3.6/site-packages/xgboost/core.py\", line 368, in __del__\n",
      "    if self.handle is not None:\n",
      "AttributeError: 'DMatrix' object has no attribute 'handle'\n"
     ]
    }
   ],
   "source": [
    "#import libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional sklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4\n",
    "\n",
    "train = pd.read_csv('/home/tyj/Jupyter_ws/xgboost_ex/dataset/Dataset/train_modified.csv')\n",
    "target = 'Disbursed'\n",
    "IDcol = 'ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Disbursed</th>\n",
       "      <th>Existing_EMI</th>\n",
       "      <th>ID</th>\n",
       "      <th>Loan_Amount_Applied</th>\n",
       "      <th>Loan_Tenure_Applied</th>\n",
       "      <th>Monthly_Income</th>\n",
       "      <th>Var4</th>\n",
       "      <th>Var5</th>\n",
       "      <th>Age</th>\n",
       "      <th>EMI_Loan_Submitted_Missing</th>\n",
       "      <th>...</th>\n",
       "      <th>Var2_2</th>\n",
       "      <th>Var2_3</th>\n",
       "      <th>Var2_4</th>\n",
       "      <th>Var2_5</th>\n",
       "      <th>Var2_6</th>\n",
       "      <th>Mobile_Verified_0</th>\n",
       "      <th>Mobile_Verified_1</th>\n",
       "      <th>Source_0</th>\n",
       "      <th>Source_1</th>\n",
       "      <th>Source_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID000002C20</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID000004E40</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35000</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID000007H20</td>\n",
       "      <td>600000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID000008I30</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>35000</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>ID000009J40</td>\n",
       "      <td>500000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100000</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Disbursed  Existing_EMI           ID  Loan_Amount_Applied  \\\n",
       "0        0.0           0.0  ID000002C20             300000.0   \n",
       "1        0.0           0.0  ID000004E40             200000.0   \n",
       "2        0.0           0.0  ID000007H20             600000.0   \n",
       "3        0.0           0.0  ID000008I30            1000000.0   \n",
       "4        0.0       25000.0  ID000009J40             500000.0   \n",
       "\n",
       "   Loan_Tenure_Applied  Monthly_Income  Var4  Var5  Age  \\\n",
       "0                  5.0           20000     1     0   37   \n",
       "1                  2.0           35000     3    13   30   \n",
       "2                  4.0           22500     1     0   34   \n",
       "3                  5.0           35000     3    10   28   \n",
       "4                  2.0          100000     3    17   31   \n",
       "\n",
       "   EMI_Loan_Submitted_Missing    ...     Var2_2  Var2_3  Var2_4  Var2_5  \\\n",
       "0                           1    ...        0.0     0.0     0.0     0.0   \n",
       "1                           0    ...        0.0     0.0     0.0     0.0   \n",
       "2                           1    ...        0.0     0.0     0.0     0.0   \n",
       "3                           1    ...        0.0     0.0     0.0     0.0   \n",
       "4                           1    ...        0.0     0.0     0.0     0.0   \n",
       "\n",
       "   Var2_6  Mobile_Verified_0  Mobile_Verified_1  Source_0  Source_1  Source_2  \n",
       "0     1.0                1.0                0.0       1.0       0.0       0.0  \n",
       "1     1.0                0.0                1.0       1.0       0.0       0.0  \n",
       "2     0.0                0.0                1.0       0.0       0.0       1.0  \n",
       "3     0.0                0.0                1.0       0.0       0.0       1.0  \n",
       "4     0.0                0.0                1.0       0.0       0.0       1.0  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意, 这里导入了 XGBoost 的两种形式:\n",
    "\n",
    "1. **`xgb`** - 我们将会使用这个库中的 **`cv`** 函数.\n",
    "2. **`XGBClassifier`** - 这使我们能够使用 sklearn 中可以并行处理的 **`GridSearchCV`** 类.\n",
    "\n",
    "下面让我们先定义一个能够帮助我们创建 XGBoost 模型和执行交叉验证的函数. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain['Disbursed'], eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['Disbursed'].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Disbursed'], dtrain_predprob))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数与我们在 GBM 中使用的函数稍有不同. 这篇文章的关注点在于涵盖概念而不是编写代码. 如果有看不明白的地方, 不要纠结. 注意: XGBoost 库的 sklearn 接口没有\"feature_importances\"指标, 但是有执行相同功能的函数 get_fscore().\n",
    "\n",
    "### 调参的一般方法\n",
    "***\n",
    "我们将在这里使用和 GBM 类似的方法, 以下是要执行的步骤:\n",
    "\n",
    "1. 选择一个相对高的学习率(learning rate). 通常0.1可以工作, 但是对于不同的问题, 学习率可能需设置在0.05-0.3之间. 接下来选择对于该学习率参数的最佳提升轮数. XGBoost 有一个非常有用的函数 \"cv\" 能够在每轮提升迭代中执行交叉验证, 从而得到所需的子估计器的最佳数目.\n",
    "\n",
    "2. 调节子估计器参数(**`max_depth`**, **`min_child_weight`**, **`gamma`**, **`subsample`**, **`colsample_bytree`**). 注意: 我们可以挑选不同的参数来定义子估计器(单颗树), 这里仅此一例.\n",
    "\n",
    "3. 调节正则化项相关的参数(**`lambda`**, **`alpha`**), 能降低模型复杂度, 提高模型性能.\n",
    "\n",
    "4. 选择更小的学习率参数, 找到最优的那个.\n",
    "\n",
    "下面让我们看一下这些步骤更详细的介绍.\n",
    "\n",
    "### Step 1: Fix learning rate and number of estimators for tuning tree-based parameters\n",
    "***\n",
    "为了确定提升参数(这里指学习率和子估计器个数), 我们需要给其他参数设定一个初始值, 让我们按照如下设置:\n",
    "\n",
    "1. **`max_depth = 5`**: 该参数应该在 3-10 之间. 我在这里选择5, 你可以尝试 4-6 之间的值.\n",
    "\n",
    "2. **`min_child_weight = 1`**: 这里选择一个很小的值, 因为这是一个类别不平衡问题, 这样叶子节点分得的样本会更少.\n",
    "\n",
    "3. **`gamma = 0`**: 也可选择 0.1-0.2 之间的值作为初始值, 该值将稍后调整.\n",
    "\n",
    "4. **`subsample, colsample_bytree = 0.8`**: 0.8是一个常用的初始值, 典型取值范围是 0.5-0.9 之间.\n",
    "\n",
    "5. **`scale_pos_weight = 1`**: 因为训练集类别高度不平衡.\n",
    "\n",
    "注意: 以上参数只是初始值, 后面会一一调节. 让我们先采用默认的学习率参数0.1, 然后使用 cv 函数找到最佳子估计器个数. 上面定义的函数可以实现这个目的."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.9854\n",
      "AUC Score (Train): 0.891681\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-0a406da40b3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m  \u001b[0mscale_pos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m  seed=27)\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodelfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-de4b8df260a1>\u001b[0m in \u001b[0;36mmodelfit\u001b[0;34m(alg, dtrain, predictors, useTrainCV, cv_folds, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUC Score (Train): %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Disbursed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain_predprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mfeat_imp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mfeat_imp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Feature Importances'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Feature Importance Score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(xgb1, train, predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以看见, 对于0.1的学习率, 最优子估计器数目是140. 注意: 该数可能大了. 如果太大, 你可以增加学习率, 然后重新运行以上代码得到更小的最优子估计器数目.\n",
    "\n",
    "### Step 2: Tune `max_depth` and `min_child_weight`\n",
    "***\n",
    "我们先调这两个参数, 因为这两个参数会对模型结果的影响最大. 我们先设置一个较宽的范围, 然后在慢慢缩小.\n",
    "\n",
    "**注意**: 这里将要执行计算量很大的网格搜索, 这可能会花费15-30分钟的时间."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.83764, std: 0.00875, params: {'max_depth': 3, 'min_child_weight': 1},\n",
       "  mean: 0.83837, std: 0.00825, params: {'max_depth': 3, 'min_child_weight': 3},\n",
       "  mean: 0.83716, std: 0.00818, params: {'max_depth': 3, 'min_child_weight': 5},\n",
       "  mean: 0.84016, std: 0.00680, params: {'max_depth': 5, 'min_child_weight': 1},\n",
       "  mean: 0.83965, std: 0.00537, params: {'max_depth': 5, 'min_child_weight': 3},\n",
       "  mean: 0.83935, std: 0.00548, params: {'max_depth': 5, 'min_child_weight': 5},\n",
       "  mean: 0.83570, std: 0.00587, params: {'max_depth': 7, 'min_child_weight': 1},\n",
       "  mean: 0.83448, std: 0.00726, params: {'max_depth': 7, 'min_child_weight': 3},\n",
       "  mean: 0.83456, std: 0.00554, params: {'max_depth': 7, 'min_child_weight': 5},\n",
       "  mean: 0.82851, std: 0.00651, params: {'max_depth': 9, 'min_child_weight': 1},\n",
       "  mean: 0.82955, std: 0.00580, params: {'max_depth': 9, 'min_child_weight': 3},\n",
       "  mean: 0.83158, std: 0.00677, params: {'max_depth': 9, 'min_child_weight': 5}],\n",
       " {'max_depth': 5, 'min_child_weight': 1},\n",
       " 0.8401574515811714)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test1 = {\n",
    " 'max_depth': list(range(3,10,2)),\n",
    " 'min_child_weight': list(range(1,6,2))\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(train[predictors],train[target])\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里测试了12种不同的参数组合, 间距为2. 最优值是 **`max_depth=5`**, **`min_child_weight=1`**. 接下来让我们进一步寻找更优的参数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.84117, std: 0.00688, params: {'max_depth': 4, 'min_child_weight': 0.1},\n",
       "  mean: 0.84057, std: 0.00670, params: {'max_depth': 4, 'min_child_weight': 1},\n",
       "  mean: 0.84122, std: 0.00667, params: {'max_depth': 4, 'min_child_weight': 2},\n",
       "  mean: 0.84016, std: 0.00529, params: {'max_depth': 5, 'min_child_weight': 0.1},\n",
       "  mean: 0.84016, std: 0.00680, params: {'max_depth': 5, 'min_child_weight': 1},\n",
       "  mean: 0.83980, std: 0.00602, params: {'max_depth': 5, 'min_child_weight': 2},\n",
       "  mean: 0.83802, std: 0.00564, params: {'max_depth': 6, 'min_child_weight': 0.1},\n",
       "  mean: 0.83877, std: 0.00519, params: {'max_depth': 6, 'min_child_weight': 1},\n",
       "  mean: 0.83896, std: 0.00565, params: {'max_depth': 6, 'min_child_weight': 2}],\n",
       " {'max_depth': 4, 'min_child_weight': 2},\n",
       " 0.8412189672353089)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test2 = {\n",
    " 'max_depth': [4,5,6],\n",
    " 'min_child_weight': [0.1,1,2]\n",
    "}\n",
    "gsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch2.fit(train[predictors],train[target])\n",
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里得到最优值是 **`max_depth=4`**, **`min_child_weight=2`**. 可以看见交叉验证得分轻微上升. 注意: 随着模型性能的提升, 提升模型的性能变得非常难. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.9854\n",
      "AUC Score (Train): 0.878673\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-3c305c5a4c84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m  param_grid = param_test2b, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n\u001b[1;32m      8\u001b[0m \u001b[0mgsearch2b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodelfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgsearch2b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mgsearch2b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgsearch2b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgsearch2b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-de4b8df260a1>\u001b[0m in \u001b[0;36mmodelfit\u001b[0;34m(alg, dtrain, predictors, useTrainCV, cv_folds, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUC Score (Train): %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Disbursed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain_predprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mfeat_imp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mfeat_imp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Feature Importances'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Feature Importance Score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "param_test2b = {\n",
    " 'min_child_weight': [2,4,6,8,10]\n",
    "}\n",
    "gsearch2b = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=4,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test2b, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch2b.fit(train[predictors],train[target])\n",
    "modelfit(gsearch2b.best_estimator_, train, predictors)\n",
    "gsearch2b.grid_scores_, gsearch2b.best_params_, gsearch2b.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.84122, std: 0.00667, params: {'min_child_weight': 2},\n",
       "  mean: 0.84034, std: 0.00601, params: {'min_child_weight': 4},\n",
       "  mean: 0.84003, std: 0.00622, params: {'min_child_weight': 6},\n",
       "  mean: 0.83889, std: 0.00714, params: {'min_child_weight': 8},\n",
       "  mean: 0.84004, std: 0.00661, params: {'min_child_weight': 10}],\n",
       " {'min_child_weight': 2},\n",
       " 0.8412189672353089)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch2b.grid_scores_, gsearch2b.best_params_, gsearch2b.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Tune **`gamma`**\n",
    "***\n",
    "现在开始调节 **`gamma`** 参数, 利用上面已经调好的参数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.84122, std: 0.00667, params: {'gamma': 0.0},\n",
       "  mean: 0.84042, std: 0.00732, params: {'gamma': 0.1},\n",
       "  mean: 0.83969, std: 0.00840, params: {'gamma': 0.2},\n",
       "  mean: 0.84038, std: 0.00707, params: {'gamma': 0.3},\n",
       "  mean: 0.83951, std: 0.00812, params: {'gamma': 0.4}],\n",
       " {'gamma': 0.0},\n",
       " 0.8412189672353089)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=4,\n",
    " min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch3.fit(train[predictors],train[target])\n",
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 可以看见, **`gamma`**的最优值是初始值. 在调节其他参数之前, 一个好的选择是重新调节子估计器个数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.9854\n",
      "AUC Score (Train): 0.887868\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-07de71752165>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m  \u001b[0mscale_pos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m  seed=27)\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodelfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-de4b8df260a1>\u001b[0m in \u001b[0;36mmodelfit\u001b[0;34m(alg, dtrain, predictors, useTrainCV, cv_folds, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUC Score (Train): %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Disbursed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain_predprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mfeat_imp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mfeat_imp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Feature Importances'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Feature Importance Score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "xgb2 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=4,\n",
    " min_child_weight=2,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(xgb2, train, predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后参数是:\n",
    "* **`max_depth=4`**\n",
    "* **`min_child_weight=2`**\n",
    "* **`gamma=0`**\n",
    "\n",
    "### Step 4: Tune **`subsample`** 和 **`colsample_bytree`**\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.83899, std: 0.00943, params: {'colsample_bytree': 0.6, 'subsample': 0.6},\n",
       "  mean: 0.83678, std: 0.00852, params: {'colsample_bytree': 0.6, 'subsample': 0.7},\n",
       "  mean: 0.83878, std: 0.00798, params: {'colsample_bytree': 0.6, 'subsample': 0.8},\n",
       "  mean: 0.83960, std: 0.00763, params: {'colsample_bytree': 0.6, 'subsample': 0.9},\n",
       "  mean: 0.84048, std: 0.00841, params: {'colsample_bytree': 0.7, 'subsample': 0.6},\n",
       "  mean: 0.84028, std: 0.00819, params: {'colsample_bytree': 0.7, 'subsample': 0.7},\n",
       "  mean: 0.83916, std: 0.00805, params: {'colsample_bytree': 0.7, 'subsample': 0.8},\n",
       "  mean: 0.84015, std: 0.00785, params: {'colsample_bytree': 0.7, 'subsample': 0.9},\n",
       "  mean: 0.84168, std: 0.00900, params: {'colsample_bytree': 0.8, 'subsample': 0.6},\n",
       "  mean: 0.83943, std: 0.00877, params: {'colsample_bytree': 0.8, 'subsample': 0.7},\n",
       "  mean: 0.84136, std: 0.00737, params: {'colsample_bytree': 0.8, 'subsample': 0.8},\n",
       "  mean: 0.84151, std: 0.00876, params: {'colsample_bytree': 0.8, 'subsample': 0.9},\n",
       "  mean: 0.83971, std: 0.00754, params: {'colsample_bytree': 0.9, 'subsample': 0.6},\n",
       "  mean: 0.83900, std: 0.00871, params: {'colsample_bytree': 0.9, 'subsample': 0.7},\n",
       "  mean: 0.84154, std: 0.00610, params: {'colsample_bytree': 0.9, 'subsample': 0.8},\n",
       "  mean: 0.84202, std: 0.00897, params: {'colsample_bytree': 0.9, 'subsample': 0.9}],\n",
       " {'colsample_bytree': 0.9, 'subsample': 0.9},\n",
       " 0.8420225779958075)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test4 = {\n",
    " 'subsample': [i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree': [i/10.0 for i in range(6,10)]\n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n",
    " min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch4.fit(train[predictors],train[target])\n",
    "gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=4, min_child_weight=2, missing=None, n_estimators=177,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=27, silent=True,\n",
       "       subsample=0.8),\n",
       "       fit_params={}, iid=False, n_jobs=4,\n",
       "       param_grid={'subsample': [0.85, 0.9, 0.95], 'colsample_bytree': [0.85, 0.9, 0.95]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test5 = {\n",
    " 'subsample':[i/100.0 for i in range(85,100,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(85,100,5)]\n",
    "}\n",
    "gsearch5 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n",
    " min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test5, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch5.fit(train[predictors],train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.84046, std: 0.00875, params: {'colsample_bytree': 0.85, 'subsample': 0.85},\n",
       "  mean: 0.84124, std: 0.01005, params: {'colsample_bytree': 0.85, 'subsample': 0.9},\n",
       "  mean: 0.84140, std: 0.01040, params: {'colsample_bytree': 0.85, 'subsample': 0.95},\n",
       "  mean: 0.84132, std: 0.00741, params: {'colsample_bytree': 0.9, 'subsample': 0.85},\n",
       "  mean: 0.84202, std: 0.00897, params: {'colsample_bytree': 0.9, 'subsample': 0.9},\n",
       "  mean: 0.84168, std: 0.00801, params: {'colsample_bytree': 0.9, 'subsample': 0.95},\n",
       "  mean: 0.84137, std: 0.00830, params: {'colsample_bytree': 0.95, 'subsample': 0.85},\n",
       "  mean: 0.84229, std: 0.00757, params: {'colsample_bytree': 0.95, 'subsample': 0.9},\n",
       "  mean: 0.84161, std: 0.00951, params: {'colsample_bytree': 0.95, 'subsample': 0.95}],\n",
       " {'colsample_bytree': 0.95, 'subsample': 0.9},\n",
       " 0.8422949410926629)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后参数是:\n",
    "* **`subsample=0.9`**\n",
    "* **`colsample_bytree=0.95`**\n",
    "\n",
    "### Step 5: Tuning Regularization Parameters\n",
    "***\n",
    "接下来应用正则化去控制过拟合. 很多人很少使用该参数, 因为 **`gamma`** 也能控制模型复杂度. 但是, 我们通常应该尝试一下, 这里我会尝试 **`reg_alpha`**, 你可以自己尝试 **`reg_lambda`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.84229, std: 0.00757, params: {'reg_alpha': 1e-05},\n",
       "  mean: 0.84131, std: 0.00721, params: {'reg_alpha': 0.01},\n",
       "  mean: 0.84199, std: 0.00751, params: {'reg_alpha': 0.1},\n",
       "  mean: 0.84186, std: 0.00794, params: {'reg_alpha': 1},\n",
       "  mean: 0.81361, std: 0.01559, params: {'reg_alpha': 100}],\n",
       " {'reg_alpha': 1e-05},\n",
       " 0.8422948494447787)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test6 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "gsearch6 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n",
    " min_child_weight=2, gamma=0, subsample=0.9, colsample_bytree=0.95,\n",
    " objective= 'binary:logistic', scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test6, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch6.fit(train[predictors],train[target])\n",
    "gsearch6.grid_scores_, gsearch6.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.84229, std: 0.00757, params: {'reg_alpha': 0},\n",
       "  mean: 0.84229, std: 0.00757, params: {'reg_alpha': 1e-06},\n",
       "  mean: 0.84229, std: 0.00757, params: {'reg_alpha': 5e-06},\n",
       "  mean: 0.84229, std: 0.00757, params: {'reg_alpha': 1e-05},\n",
       "  mean: 0.84229, std: 0.00757, params: {'reg_alpha': 5e-05}],\n",
       " {'reg_alpha': 0},\n",
       " 0.8422949410926629)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test7 = {\n",
    " 'reg_alpha':[0, 1e-6, 5e-6, 1e-5, 5e-5]\n",
    "}\n",
    "gsearch7 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n",
    " min_child_weight=2, gamma=0, subsample=0.9, colsample_bytree=0.95,\n",
    " objective= 'binary:logistic', scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test7, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch7.fit(train[predictors],train[target])\n",
    "gsearch7.grid_scores_, gsearch7.best_params_, gsearch7.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.9854\n",
      "AUC Score (Train): 0.888192\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-3fa67f5e6f2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m  \u001b[0mscale_pos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m  seed=27)\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodelfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-de4b8df260a1>\u001b[0m in \u001b[0;36mmodelfit\u001b[0;34m(alg, dtrain, predictors, useTrainCV, cv_folds, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUC Score (Train): %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Disbursed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain_predprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mfeat_imp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mfeat_imp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Feature Importances'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Feature Importance Score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "xgb3 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=4,\n",
    " min_child_weight=2,\n",
    " gamma=0,\n",
    " subsample=0.9,\n",
    " colsample_bytree=0.95,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(xgb3, train, predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Reducing Learning Rate\n",
    "***\n",
    "最后, 我们应该减小学习率, 这样会得到更多子估计器."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.9854\n",
      "AUC Score (Train): 0.884858\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-5bdb3d5cd043>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m  \u001b[0mscale_pos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m  seed=27)\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodelfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-de4b8df260a1>\u001b[0m in \u001b[0;36mmodelfit\u001b[0;34m(alg, dtrain, predictors, useTrainCV, cv_folds, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUC Score (Train): %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Disbursed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain_predprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mfeat_imp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mfeat_imp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Feature Importances'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Feature Importance Score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "xgb4 = XGBClassifier(\n",
    " learning_rate =0.01,\n",
    " n_estimators=5000,\n",
    " max_depth=4,\n",
    " min_child_weight=2,\n",
    " gamma=0,\n",
    " subsample=0.9,\n",
    " colsample_bytree=0.95,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(xgb4, train, predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "这里, 我想分享两个关键点:\n",
    "\n",
    "1. 仅仅通过调参和使用稍好的模型很难获得很大的性能提升. \n",
    "2. 想要更大的性能提升, 可以尝试一下: 特征工程(feature engineering), 模型集成(ensemble of models), stacking 等等."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
