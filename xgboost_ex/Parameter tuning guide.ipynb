{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 调参小结\n",
    "***\n",
    "参数调节在机器学习中是一门艺术, 一个模型的最优参数依赖于不同的场景. 所以提供一个指南是很有必要的.\n",
    "本文试图给使用 XGBoost 库时的调参提供一些指导.\n",
    "\n",
    "## 1. 理解偏差-方差折衷(Understanding Bias-Variance Tradeoff)\n",
    "***\n",
    "如果你上一门机器学习或者统计学的课程, 这个概念很可能是最重要的概念之一. 当我们使我们的模型变得更加复杂时(例如:树的深度更深), 模型有更强的能力去拟合训练数据, 会得到一个偏差更低的模型. 然而, 更复杂的模型需要更多的训练数据去学习.\n",
    "\n",
    "XGBoost 中的大部分参数是关于偏差-方差折衷的. 最好的模型应该仔细权衡兼顾模型复杂度和预测能力两者. [XGBoost 参数文档](http://xgboost.readthedocs.io/en/latest/parameter.html)将告诉你每个参数对模型的影响. 这有助于你选择适度复杂的模型.\n",
    "\n",
    "## 2. 控制过拟合(Control Overfitting)\n",
    "***\n",
    "当你发现模型在训练集上取得高精度, 但是在测试集上表现很差时, 很可能是模型过拟合了.\n",
    "在 XGBoost 中通常有两种方法控制模型过拟合:\n",
    "*   第一种方法是直接控制**模型复杂度**\n",
    "    > 包括: `max_depth`, `min_child_weight` 和 `gamma`\n",
    "*   第二种方式是添加**随机性**使模型训练时鲁棒性更高\n",
    "    > 包括: `subsample`, `colsample_bytree`\n",
    "    >\n",
    "    > 你也可以减小步长 `eta`, 但是记住同时需要增加 `num_round`\n",
    " \n",
    "## 3. 处理不平衡的数据集(Handle Imbalanced Dataset)\n",
    "***\n",
    "对于常见情况, 例如广告点击日志, 数据集是非常不平衡的. 数据集不平衡会影响 XGBoost 模型的训练, 这里有两种方法去改善:\n",
    "*  如果你仅仅只关注你的预测结果的排序(AUC)\n",
    "   > 平衡正例和反例权重, 通过 `scale_pos_weight`\n",
    "   >\n",
    "   > 使用AUC来评估模型\n",
    "* 如果你关注预测正确的概率\n",
    "    > 这时你不能重新平衡(re-balance)数据集\n",
    "    > \n",
    "    > 这时设置 `max_delta_step` 为一个有限值(比如说1)会帮助训练收敛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
